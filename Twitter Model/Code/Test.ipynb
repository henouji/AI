{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 11:52:53.883498: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-01 11:52:53.930905: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-01 11:52:53.931656: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-01 11:52:54.739943: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "with open(\"../Data/Tweets.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '\"a', '\"akala', '\"alam', '\"all', '\"ang', '\"anger\"', '\"ano?', '\"aquino', '\"bakit', '\"be', '\"confident', '\"crossini', '\"don\\'t', '\"flexible', '\"get', '\"happy', '\"i', '\"i\\'m', '\"ikaw', '\"kulang', '\"liwanag\"', '\"lover\"', '\"nasasakal', '\"nothing\"', '\"para', '\"picturan', '\"piliin', '\"please', '\"pm', '\"sana', '\"seriously,', '\"shithole\"', '\"special\"', '\"tangina', '\"the', '\"things\"', '\"think', '\"tulungan', '\"wala', '\"what', '\"what\\'s', '\"why?\"', '\"wouldn\\'t', '#bdonomura', '#buhatbayag', '#chu2koi', '#nowplaying', '#oregairu', '#spotifywrapped', '#sub2pewds', '#subscribetopewdiepie', '#‰ø∫„Ç¨„Ç§„É´', '#Á•ù‰ø∫„Ç¨„Ç§„É´Á¨¨3Êúü', '%', '&amp;', '&lt;3', \"'ako'\", \"'autshatfisisciarnme\", \"'bout\", \"'could\", \"'di\", \"'em\", \"'healthy\", '\\'ko.\"', '\\'tayo\\'?\"', \"'to\", \"'to.\", '\\'tomozaki-kun\\'\"', \"'tong\", \"'wmpe/adil]3willsmüåéhurjyb.\", '(yes)', '*bought', '*have', '*mind', '*still', ',', '-', '-alanis,', '-zero-', '.', '...', '._.', '0.2332', '0.2834', '0.5', '1', '1.', '10', '12', '140', '167', '2', '2.', '2.7875', '2019', '2021', '2022', '2022.', '22', '27;', '2:', '2nd...', '3', '3,440', '3.', '3.!!!!!!!!', '3aver', '3r...4th', '4', '5', '500php.', '6/10', '7:46', '8', '8he', ':)', ':[', ':c', ':d', ';-;', ';3;', '=', '@9gag', '@adodoots', '@greyunitato', '@taylorswift13', 'a', 'aaaaaaaaaaaaaaa', 'aaaaaaaaaaaaaaaaaaaaaaaaaaaa', 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\"', 'aaaaaaaahghhhhhhhleluya', 'aalis', \"abc's\", 'abel', 'able', 'about', 'absolutely', 'abused', 'accept', 'according', 'account', 'account?', 'achieved...', 'achievement.', 'achievements', 'achievements?', 'acted', 'acting', 'action', 'actually', 'actually,', 'adhd', 'adjustments.', 'admin', 'administration\"', 'admire', 'adult', 'advantage', 'af', 'affect', 'afford', 'afloat.', 'afraid.', 'after', 'agad', 'again', 'again!', 'again,', 'again.', 'again?', 'age,', 'agin', 'ago', 'ago.', 'ah', 'aha', 'ahasip', 'ahhhhh', 'ahhhhhhh.', 'ahleuya', 'ai', 'ai,', 'ai.', 'aiet', 'aight', 'aight.', \"ain't\", 'airor', 'aka!', 'akala', 'ako', 'ako,', 'ako.', 'ako?', 'akong', 'aksygk3.kv8bihas', 'alam', 'alarm', 'album', 'alis', 'all', 'all.', 'almost', 'alo', 'along.', 'already', 'already.\"', 'also', 'also,', 'always', 'always.', 'am', 'am.', 'ampota.', 'an', 'an-sikip\"', 'and', 'android', 'ang', 'angry,', 'angular', 'angularfire2', 'anime', 'annoying.', 'ano', 'ano?', 'anong', 'another', \"another's\", 'another.', 'answer.', 'anxiety', 'anxiety.', 'any', 'anyare?', 'anymore', 'anymore.', 'anymore?', 'anyone', 'anything', 'anything.', 'anyways', 'anywhere.', 'app!', 'app,', 'apparently', 'apparently,', 'appeal', 'apple.', 'appreciated.', 'aq', 'ar', 'ara', 'arat.', 'araw', 'araw.', 'are', 'are.', 'argue', 'arguing', 'argument?', 'around.', 'arrived', 'as', 'asan', 'ask', 'asked', 'asking', 'assitant?', 'assume.', 'at', 'attention', 'attest', 'attire.', 'aughosting', 'august', 'author', 'auto-complete.', 'average', 'aware', 'awat', 'away', 'away.', 'away:', 'awful', 'awkward', 'ay', 'ayoko', 'ayop', 'ayun', 'ayun,', 'b', \"b'os\", 'ba', \"ba'y\", 'ba?', 'back', 'back.', 'backer.', 'backlash', 'bad', 'bad.', 'bagong', 'bakit', 'balls,', 'balls.', 'banishment!', 'barely', 'baseless', 'basically', 'bath', 'bato', 'battle', 'bawling', 'be', 'be.', 'beatles', 'bebu.', 'became', 'because', 'become', 'bedeoo', 'bee:', 'beeg', 'been', 'been.', 'before', 'begging', 'behaviours.', 'behind', 'being', 'belief', 'believe', 'ben', 'best', 'best.', 'better', 'better.', 'bg-kun', 'biglang', 'birth', 'birthday', 'birthday.', 'birthdays', 'bit', 'bitch.', 'bite', 'bitter', 'bitterness', 'black', 'blame', 'blasted', 'blind', 'blinded', 'bliss!', 'bloating.', 'blockchain', 'blocks', 'bloodborne...', 'blue;', 'blurred.', 'boat', 'bobo', 'bobo.', 'body', 'boi', 'boisterous', 'book', 'books', 'borderline', 'both', 'bottle', 'bought', 'boy', 'boyz', 'brain', 'brain.', 'brb.', 'break.', 'breakdown', 'breath', 'breathing', 'bright', 'broke', 'browsing', 'bruh', 'bruh.', 'bs', 'bu', 'bucket', 'buhay', 'build', 'bukas', 'bukas.', 'bullshit', 'bumble', 'bunko', 'burn', 'burned', 'bus', 'busy', 'but', 'but...', 'buti', 'buy', 'buying', 'bwahahahahahahahhahah', 'by', 'bye', 'bye?', 'cahu.', 'calculation', 'call', 'called', 'calm', 'came', 'camille,', 'can', \"can't\", \"can't.\", 'cap', 'capital.', 'card', 'card.', 'care.', 'career', 'career.', 'casted', 'cause', 'cave', 'celebrate', 'certainty.', 'chair', 'challenging.', 'champagne', 'change', 'change.\"', 'changed', 'changes.', 'character', 'characters.', 'charismatic', 'chat.', 'chats', 'check', 'cheesy?', 'childish', 'chiyo-nee', 'choice,', 'chuunibyou', 'citing', 'clarity.', 'clean', 'clearly', 'cliff', 'clone?', 'close', 'clouded.', 'clue', 'code', 'code.', 'cokainbiteerssy', 'collab', 'collecting', 'colors,', 'come', 'comes', 'command', 'commend', 'competence.', 'complicated', 'condescending', 'confessed', 'confidence', 'confirmation.', 'confirmed', 'confused', 'confused.', 'confusion', 'congratulated', 'congratulations', 'constant', 'container', 'contemplate', 'context', 'continue', 'contributors', 'control', 'conversational', 'convincing.', 'cookie', 'copy', 'corporal', 'corporate', 'could', \"could've\", \"couldn't\", 'couldn‚Äôt', 'course.', 'cover', 'create', 'credit', 'cry', 'crying', 'curse', 'cuz', 'dabbled', 'dahil', 'dalawa', 'dama', 'dami', 'damn', 'damn.', 'damned', 'damning', 'damnit!', 'dapat', 'das', 'dataset', 'daveaa\"', 'david', 'david\"', 'david,', 'david.', 'daw', 'day', 'day!', 'day,', 'day.', 'days', 'days?!', 'dead', 'dead.', 'deadline.', 'deal', 'dearest', 'debugging', 'ded', 'deep', 'deeply', 'definite', 'definitely', 'degree', 'demanding', 'demeaning', 'demo', 'depresed', 'depressed', 'depression', 'depression.', 'despite', 'destroying', 'dev', 'developed!', 'devices', 'di', 'diagnose', 'diagram', 'dialogflow', 'did', \"didn't\", 'die', 'different', 'differentiation', 'din', 'din.', 'directly', 'diretsuhin', 'disappoint', 'disappoint?', 'disappointed', 'disappointed.', 'disappointing.', 'disappointment', 'disgusting', 'disparity', 'disregard', 'dito', 'dlc', 'do', 'do?', 'doc!', 'doctor:', 'does', 'does.', \"doesn't\", 'doing', 'doing?\"', 'domain...', \"don't\", \"don't.\", 'done', 'done.', 'dont', 'doodoo', 'doubt', 'down', 'down,', 'down.', 'down:', 'draw.', 'dream', 'dream.', 'dreams', 'dressed', 'drink/', 'drinking', 'drown', 'duck', 'dude', 'dude.', 'due', 'duh', 'dum-dum.', 'dumb', 'dumb.', 'dumb?', 'dumbass', 'dumber', 'dumbest', 'dumdum', 'dun', 'dun.', 'duuuuudee...', 'duuuuuh????', 'duuuuuhhh????!!!', 'e', 'e.', 'each', 'early', 'early.', 'earphones', 'ears,', 'eat', 'eating', 'edi', 'edi...', 'editing', 'eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee', 'eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeep', 'effort', 'efforts', 'eggs', 'ego', 'eh', 'ehe...', 'ehem.', 'either', 'ekew', 'else', 'else.', 'email', 'empathize', 'empty', 'end', 'end,', 'end.', 'energy', 'eng', 'enjoy', 'enjoy.', 'enormous.', 'enough', 'enough.', 'entirety', 'environment', 'environment.', 'eoh', 'epic.', 'epoch', 'era', 'erew', 'errthings', 'estimate', 'estopace', 'ethical', 'ethics', 'eto', 'even', 'event', 'ever.', 'every', 'everyone', 'everything', 'everytime', 'ex?', 'exactly', 'exactly.', 'exception.', 'excited', 'excruciating', 'excuse', 'exercise.', 'exerting', 'exes.', 'existence.', 'expect', 'expecting', 'expecting.', 'experience', 'experiments', 'explanation', 'exploited,', 'exploits', 'eyes', 'eyyy', 'facebook', 'facebook.', 'facial', 'fail.', 'failing', 'fails.\"', 'failure...', 'faken', 'fallen', 'fam', 'fam.', 'family', 'famous', 'fangirl', 'fast?', 'fault', 'fck', 'fck.', 'fck..', 'fck?', 'fcks.', 'fear', 'features', 'feeble', 'feel', 'feel.', 'feeling', 'feelingcutemightdeletelater', 'feelings', 'feelings.', 'feels', 'fell', 'felt', 'feng', 'ffffffffffuuuuu...', 'finally', 'fine', 'fine.', 'fireworks,', 'first.', 'fit', 'fitting.', 'fix', 'fixing', 'flag.', 'flores:', 'flow?', 'fml', 'focus', 'focused', 'food', 'for', 'for.', 'for?', 'foresee', 'foresee.', 'forget', 'forget.', 'forgot', 'forgotten', 'forward', 'forward.', 'found', 'fouqing', 'four', 'fowkin', 'free', 'freedom', 'fresh', 'fret', 'frick', 'friend', 'from', 'frustrations', 'fu...', 'fuck', 'fuck.', 'fucked', 'fucked.', 'fucken', 'fucker.', 'fuckin', 'fucking', 'fucking.', 'fucking...', 'fuckit.', 'fucks', 'fucks.', 'fugging', 'fukcin', 'fulfill', 'fulfill,', 'fulfillment.', 'fully', 'fuq', 'furniture', 'future.', 'fuuuuckk....', 'fux.', 'fuxk', 'g', 'gaaaaaa', 'gag*.', 'gagaga', 'gago', 'gago,', 'gago.', 'gahd', 'galit', 'game', 'game!', 'game,', 'game.', 'games.', 'ganda', 'ganito', 'ganito.', 'ganito?', 'ganun?', 'ganyan', 'gave', 'gawin', 'gaya', 'geh.', 'gentle', 'get', 'get?', 'gets', 'gets.', 'getting', 'gift', 'gift.', 'girl!', 'girlfriend', 'give', 'given', 'giving', 'go', 'go.', 'go?', 'goal', 'goal.', 'goals', 'god', 'god.', 'godawful', 'goddamn', 'goddamnit,', 'goddamnit.', 'goes', 'going', 'gone.', 'gonna', 'good', 'good.', 'google', 'got', 'gotten', 'gotten.', 'gradually', 'grasp', 'grateful', 'great', 'greatest', 'greet', 'greeting', 'grew', 'grow:', 'growing', 'guess', 'gusto', 'guy,', 'guy.', 'haa...', 'haaa...', 'haaaa...', 'haaaaa...', 'haahhahahah', 'habang', 'had', 'hahabol', 'hahah', 'hahaha', 'hahaha...', 'hahahah', 'hahahah.', 'hahahaha', 'hahahaha.', 'hahahahaha', 'hahahahahah', 'hahahahatdog.', 'hahahha', 'hahahhaha.', 'hahahhahahahah', 'hajiker', 'hajikero', 'halagily', 'halata', 'halt', 'hands', 'hangad', 'hapies', 'happened.', 'happens', 'happens.', 'happy', 'happy.\"', 'happyness', 'hard', 'hard.', 'harmful', 'harrington.', 'harurot', 'has', \"hasn't\", 'hate', 'hates', 'hating', 'hatred.', 'have', 'have.', \"haven't\", 'having', 'hay', 'hayup', 'hazero', 'he', 'head', 'headphones', 'healing', 'hear', 'heard', 'heart.\"', 'heavenly', 'hehe...', 'hell', 'hell.', 'helllooooo???', 'help', 'help.', 'help?', 'hen', 'her', 'her,', 'her.', 'here', \"here's\", 'here.', 'here?', 'hey', \"hhagamn.wdmfeea:)rpgbidcg'*Á•ù.\", 'hi', 'hi,', 'hide.', 'high', 'hihi', 'hihi.', 'hiiiiii', 'hindi', 'hirap', 'history.', 'hit', 'hmmmmm', 'hnj', 'holaaaaaaaa', 'holding', 'holes', 'homage', 'home', 'honesty,', 'hope', 'hopeful', 'hoping', 'horn', 'horns.', 'hours', 'how', 'how.', 'how2parent', 'https://t.co/1guinsyd7b', 'https://t.co/54nsor8jbp', 'https://t.co/7cqgdhoi7d', 'https://t.co/7dzlrao8uq', 'https://t.co/8p5jbg2pdy', 'https://t.co/bdx6b0dvzy', 'https://t.co/bgm7znhpu6', 'https://t.co/bjvdjekfou', 'https://t.co/bmogis8qxj', 'https://t.co/busoar9xyr', 'https://t.co/c7jmhqlt7n', 'https://t.co/ehozljrpy7', 'https://t.co/end0b3bd0h', 'https://t.co/fekewo57jj', 'https://t.co/fqqjnvmjc6', 'https://t.co/hsald3awyw', 'https://t.co/iehj2n3uul', 'https://t.co/iikl6ylqhi', 'https://t.co/isoz7g6k7v', 'https://t.co/jkkbko5ivu', 'https://t.co/knupandjcj', 'https://t.co/l7blfd12xv', 'https://t.co/loi6ju6evr', 'https://t.co/n8gajpvicg', 'https://t.co/o3zmy8wpud', 'https://t.co/odfwtlflej', 'https://t.co/p2nzt6fqtf', 'https://t.co/peg2xebld2', 'https://t.co/rfskkj4bkh', 'https://t.co/rscvwoy7vb', 'https://t.co/sdsmjkyyn8', 'https://t.co/sfi5pdbiuv', 'https://t.co/skb1segm28', 'https://t.co/thjiec5odf', 'https://t.co/vfwbvkxrcu', 'https://t.co/vsv4xod9tu', 'https://t.co/wf85phbq1k', 'https://t.co/wkydgvu9uu', 'https://t.co/x8xmlpcaej', 'https://t.co/xakwktdfp0', 'https://t.co/xuq53fxzfr', 'https://t.co/xyjtsmrurl', 'https://t.co/z15jicusww', 'https://t.co/zs3qllv5oz', 'huh?', 'human', 'human.', 'hurts', 'hurts.', 'hvae', 'hypocritical', 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'i,', 'i-let', 'i-post.', 'i...', 'i.takeios', 'i?', 'iba', 'iba-ibang', 'iba?', 'idea', 'idea.', 'idealist', 'ideas', 'idek', 'idiot.', 'idk.', 'ienb', 'if', 'ignorance', 'ignorant', 'ih,', 'ih.', \"ika'y\", 'ikalma', 'ikaw', 'ikaw,', 'ikinahihiya?', 'ilagay', 'ilugar', 'im', 'images.', 'imagine', 'imma', 'impatient', 'importance', 'important', 'important.', 'in', 'in.', 'inaasahan', 'inarte', 'incapable', 'indeed,', 'independence', 'inentertain', 'inept.', 'ineptitude', 'infamous', 'info!', 'information', 'ingrained.', 'iniisip', 'init', 'inom', 'insecure', 'insecure.', 'insecure.\"', 'insecurities', 'inside', 'insta-fame', 'instead', 'intense', 'intensifies.', 'interact', 'into', 'invalidated.', 'invalidates', 'involved', 'ionic', 'ipag', 'ipapakita', 'ipinaglalaban', 'irohasu', 'irony', 'irreversible', 'is', 'is,', 'isa', 'isakripisyo', 'isang', \"isn't\", 'issues.', 'it', 'it!!!!', \"it's\", 'it.', 'it.\"', 'it...', 'it?', 'itaga', 'itatago.', 'item', 'ito', 'its', 'itself.', 'i‚Äôll', 'i‚Äôm', 'jacket.', 'japanese.', 'jar.', 'jazz?', 'jealous', 'jessica.', 'jesu*fck', 'jetp', 'job.', 'jocelyn', 'jowa', 'jowa.', 'joy?', 'juice,', 'junior.', 'just', 'just.', 'just...', 'k-pop', 'k-pop.', 'ka', 'ka,', 'ka.', 'kaggle', 'kagit', 'kahit', 'kaibigan', 'kailan', 'kailangan', 'kakapeepee', 'kalinawan', 'kalinawan.', 'kami\"', 'kami,', 'kaming', 'kanata', 'kang', 'kanya', 'kape', 'kape...', 'kasi', 'katabi', 'kausap', 'kay', 'kaya', 'kaya?', 'kayo', 'kayo.', 'kaysa', 'keep', 'kese', 'keshi', 'kewl', 'key\".', 'khai', 'khp', 'kick', 'kicking', 'kicks', 'kill', 'killing', 'killl', 'kilos.', 'kinda', 'kingina', 'kita\"', 'kita.', 'kita?', 'klase', 'knew', 'knife', 'know', 'know.', 'know...', 'ko', 'ko,', 'ko.', 'ko.\"', 'ko?', 'kompirmasyon?', 'kontabida.', 'koto.', 'ku', 'kulang', 'kulang.', 'kulang?', 'kumukuha', 'kung', 'kyoukai', 'labas', 'labasan', 'label', 'label,', 'labo', 'lack', 'lacking', 'lagi', 'laging', 'lahat', 'lahat.', 'lakas', 'lande', 'lang', 'lang.', 'language', 'laravel,', 'laravellll', 'large', 'laro', 'last', 'last.', 'lastly', 'late', 'later.', 'laughter', 'learned', 'learning!', 'least', 'leave', 'leaving', 'led', 'left', 'left.', 'lege', 'legit', 'lele', 'lemme', 'less', 'let', \"let's\", 'letse', 'letting', 'let‚Äôs', 'lied', 'liek', 'life', 'life.', 'life?', 'life?‚Äù', 'like', 'like.', 'liked', 'liking', 'line', 'list:', 'lista', 'listen', 'listening', 'little', 'live', 'live,', 'live.', 'living', 'living,', 'llm', 'lmag', 'lmao', 'lol', 'lol.', 'loner', 'long', 'longer?', 'loob.', 'look', 'looking', 'looks', 'lose', 'loses', 'loss', 'lost', 'lot', 'lots', 'loud', 'louder', 'love', 'love.', 'low', 'lowest.', 'lucky', 'lugmok', 'luh?', 'lul', 'lumugar.', 'luya', 'lvw', 'lwee', 'ma-a-address', 'ma-t-tag', 'maaga.', 'mabibingi', 'mad', 'made', 'made...', 'mag', 'mag-isa.', 'magaaway', 'magic.', 'magiging', 'maginay', 'maging', 'magisa', 'magrant', 'magsalita.', 'magseselos?', 'magteteleport', 'mah', 'mahinang', 'maiintindihan', 'main', 'mainit', 'maka-throwback', 'makakuha', 'makapakinig', 'makausap', 'makausap.', 'make', 'make!', 'makes', 'makikita?', 'making', 'makita', 'malabo?', 'malaki', 'malala.', 'malilimutan', 'malungkot', 'malungkot?', 'mami\"', 'man', 'man.', 'man?', 'manage', 'manga', 'manga,', 'manufacture', 'many', 'mapapa-install', 'mark', 'market', 'martin.', 'martin?', 'mas', 'masaet', 'masaya', 'masaya.', \"master's\", 'material.', 'matix.', 'mature.', 'mature...', 'maxime', 'may', 'mayaman', 'maybe', 'mc', 'me', 'me,', 'me.', 'me...', 'me:', 'me?', 'mean', 'means', 'meant', 'media', 'medyo', 'medyo.', 'meelc', 'meet', 'meh,', 'meme,', 'meme-ing', 'memory', 'mention.', 'merge', 'mesi', 'met', 'meta', 'mf', 'mga', 'mhy.nq', 'mi', 'might', \"might've\", 'milestone', 'mind', 'mind,', 'mind.', 'minded', 'minute', 'minutes', 'mirai', 'misery', 'mismo', 'miss', 'missed', 'missing', 'mistakes', 'mixed', 'mny', 'mo', 'mo\"', 'mo,', 'mo.', 'mo.\"', 'mo?', 'moar', 'mobile', 'model', 'model.', 'momaaaaa', 'moment', 'money', 'monkey', 'months', 'mood,', 'mood.', 'mood?', 'mooooo', 'more', 'more.', 'more....', 'moriset.', 'most', 'motivation', 'motor', 'motor.', 'moved', 'movement', 'movie', 'moving', 'mowawyo', 'mtimkeetobeox0e', 'much', 'much!!!', 'much.', 'muhself', 'mukha', 'mulling', 'multiplayer', 'mum', 'muna?\"', 'mundane', 'music', 'my', 'myday', 'myself', 'myself.', 'myself?', 'n', 'n0t', 'n://t.co/ilniryboij', 'na', 'na,', 'na.', 'na?', 'naaaaaa', 'nadamay', 'nag', 'nag-ipo', 'nagbigay', 'naging', 'nagkita', 'nagmamaka-awa', 'nagtatanong.', 'nagusap.', 'nagyaya', 'nah', 'nahihilo,', 'naiintindihan', 'naiwan', 'nakabati', 'nakakabasa', 'nakakahiya', 'nakakaintindi', 'nakakairita', 'nakakaumay', 'nakalimutan', 'nakikilala', 'nakilala', 'nakumpleto', 'nalaman', 'nalilito', 'naman', 'naman.', 'namarn', 'name', 'name.', 'named', 'namroblema', 'nanaman', 'nanaman?', 'nanaman?\"', 'nandito', 'nandyan', 'naol', 'nap', 'napaka', 'nararamdaman.', 'nasa', 'nasasabihan', 'nasira', 'native', 'natural.', 'nauseas.', 'nawalan', 'need', 'need.', 'needed', 'needs', 'nemen', 'nerds', 'nervous', 'neto?', 'netong', 'neural', 'never', 'never.', 'nevermind', 'new', 'new.', 'next', 'ng', 'nga', 'ngayon', 'ngayon.', 'ngunit', 'ni-let', 'nice', 'nice.', 'nightmare', 'nila', 'nilabelan', 'nlex', 'no', 'no.', 'no...', \"nobody's\", 'nobody:', 'noel,', 'noes...', 'non-existent', 'non-invertible', 'non-sentient', 'nonetheless', 'nonetheless.', 'noon', 'nope.', 'normally.', 'not', 'not.', 'note', 'nothing', 'nothing.', 'nothing?', 'notice', 'notion', 'now', 'now,', 'now.', 'nrid', 'nu', 'nun', 'nun.', 'nung', 'nya', 'nyo', 'o', 'o/./hs', 'o@lwymisa', 'observing.', 'oei', 'oeuhtiksc', 'of', 'of.', 'off', \"off'\", 'off.', 'ofrsalyietawit', 'oh', 'oh.', 'okay', 'okay,', 'okay.', 'oks', 'oks.', 'old', 'omks.', 'on', 'on.', 'once', 'one', 'one.', 'onee', 'ones', 'online', 'only', 'onto', 'oof.', 'oops', 'ope\"', 'open', 'opinion:', 'opportunity', 'or', 'or...', 'order.', 'oregairu', 'other', 'other,', 'other?', 'oud', 'oumay', 'our', 'our,', 'ousee', 'out', 'out.', 'over', 'overfit', 'overthinking.', 'own', 'owo)', 'owo)/', 'p', 'p-provide', 'pa', 'pa?', 'pag', 'page', 'pagiging', 'pagkukulang', 'pagod', 'pagpili', 'paid', 'pain,', 'pait', 'pake', 'pake.', 'pake?', 'pala', 'pala.', 'paminsan', 'pandas??!!', 'pandemic', 'pang', 'papakita', 'papakitang', 'para', 'parang', 'part', 'part.', 'pasaring', 'pass', 'past', 'past.', 'pc', 'peace', 'pent', 'people', 'people.', 'permission', 'pero', 'person', 'person,', 'person.', 'personal', 'personality,', 'petty.', 'pewdiepie', 'phd', \"philippines',\", 'phrase', 'pictures', 'pill', 'pinagtarayan', 'pinakadisappointed.', 'pinasok', 'pinoproblema?', 'pipiliin', 'pipilin', 'place.', 'placed', 'placement', 'plan', 'plane', 'plans?', 'play', 'played', 'playing', 'please', 'please!!', 'please.', 'please?', 'plith.', 'plot', 'pls.', 'po', 'point.', 'point?', 'pointless', 'pokemon.', 'political', 'poopoo', 'popularity', 'position.', 'post', 'post.', 'posting', 'potato', 'potential', 'powerpoint', 'pp.', 'ppppffffffck.', 'prain', 'prediction.', 'prefer', 'premium.', 'prepare', 'present', 'pressure\"', 'pretend', 'pride', 'problem', 'problema', 'problems.', 'process', 'product', 'professional', 'programming', 'progress', 'progress,', 'project', 'project,', 'promotion', 'properly', 'proportional', 'proud', 'provide', 'ps5', 'ptuleipar', 'puqinang-qinang', 'purchase', 'pure', 'pursue', 'push', 'put', 'puta', 'putangina.', 'pwede', 'pwede?', 'quantify', 'question', 'quick!', 'quick.', 'quite', \"ra'lad\", 'rain', 'random', 'random.', 'rant', 'rant.', 'ranting', 'rather', 'rational', 'react', 'react-js', 'read', 'read...', 'ready', 'real', 'real!', 'reality', 'realization', 'really', 'really.', 'reason', 'reason,', 'reasons.', 'rebuild', 'recently,', 'recess', 'recognition', 'recreate', 'recurring', 'red', 'red;', 'redefining', 'redoing', 'reee.', 'reeee', 'reeeeeee', 'reeeeeeeee', 'reeeeeeeeee', 'reeeeeeeeeeeee', 'reeeeeeeeeeeeeeeee', 'reeeeeeeeeeeeeeeeee', 'reeeeeeeeeeeeeeeeeeeeeee', 'reeeeeeeeeeeeeeeeeeeeeeeeeeeeeee', 'reeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee', 'regret', 'rejected', 'rejection', 'relations', 'relationship', \"relationship'?\", 'relationship.', 'relationship.\"', 'release', 'releasing', 'remember', 'remember.', 'remembering', 'remind', 'reminded', 'reminder:', 'reminds', 'remove', 'removing', 'render', 'reply', 'research', 'reso?', 'respect.', 'responsibilities', 'resume', 'return', 'review', 'review:', 'revolve', 'rewards.', 'ride', 'rift', 'right', 'right.', 'right?', 'risk.', 'risks', 'rita', 'rn', 'rn.', 'robert', 'romantic', 'roses', 'ruined', 'run', 's-sea', 'sa', \"sa'kin\", \"sa'kin.\", 'sa\\'kin.\"', \"sa'yo\", 'sa\\'yo\"', \"sa'yo.\", 'sa\\'yo?\"', 'sabi', 'sabihin', 'sad', 'sad.', 'saet', 'safe', 'said', 'said.', 'saka', 'sakit', 'sakit.', 'sakto-sakto', 'salamat', 'salary', 'salasaa!', 'saloobin.', 'sama', 'same', 'samedt.', 'san', 'sana', 'sane!', 'sanity.', 'sapat', 'sarili', 'sasabihin', 'sauce', 'save', 'say', 'say.', 'say:', 'saya', 'sayo', 'sayo.\"', 'scale', 'scalpers.', 'scared', 'scared.', 'scares', 'scary', 'scenario', 'scratch', 'screen', 'season', 'seat', 'second', 'seconds', 'secs', 'sedtpri', 'see', 'seems', 'self', 'self.', 'self:', 'selfish', 'selfish.', 'sell', 'sementeryo.', 'send', 'sending', 'senpai', 'sense', 'sentence.', 'sentient', 'september', 'seq2seq', 'serious', 'seriously', 'seriously,', 'service.', 'services.', 'set', 'settle', 'setup', 'shadow', 'shallow', 'shameful', 'shangingangnang', 'shared.', 'she', \"she's\", 'sheer', 'shell.', 'shieeeet', 'shiny', 'shit', \"shit's\", 'shit,', 'shit.', 'shots', 'should', \"should've\", 'should.', \"shouldn't\", 'show', 'shown', 'showy?', 'shui', 'shunga.', 'shut', 'shuta', 'shy', 'si', 'sifted', 'sige', 'sige.', 'sighs', 'sign', 'significant', 'siguro', 'sila', 'silence', 'silent.', 'simply', 'sinabi', 'sinagot', 'since', 'sinetch', 'single', 'sino', 'sir.', 'sit', 'site', 'situation', 'sive', 'skills?', 'sleeeeeeeeeee...', 'sleep', 'sleep.', 'sleeping', 'slip', 'slowly', 'small', 'smart', 'smarter', 'smile', 'smthn', 'snappening', 'so', 'so...', 'social', 'sociopath.', 'solace', 'soldier', 'solitude', 'solve', 'som', 'some', 'someday', 'someday,', 'somehow', 'someone', 'someone.', 'something', 'something,', 'something.', 'somewhere?', 'song', 'song.', 'soon', 'sorry', 'sorry.', 'sorry?', 'soul.', 'source', 'soy', 'soy!!!!', 'space', 'special', 'spelled', 'spend', 'spending', 'spent.', 'spite', 'spiteful', 'sponsor', 'spotify', 'spreads', 'sss', 'st', 'stages?', 'stahp', 'stand', 'stand.', 'standard?', 'staring', 'start', 'start.', 'started', 'starting', 'state', 'status.', 'stay', 'stay.', 'steal', 'steep', 'steer', 'steve', 'stfu', 'still', 'still.', 'stone,', 'stones.', 'stop', 'stop.', 'stopped', 'story', 'story.', 'straight', 'stress', 'stressed', 'structurereeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee', 'stuck', 'study', 'study.', 'stuff', 'stupid', 'stupid.', 'style', 'su', 'subconscious', 'subjecting', 'succ', 'success', 'such', 'suck', 'sucked', 'sucky', 'suddenly,', 'suffer', 'suggestions', 'suggestions?', 'suicidal', 'suicidal?', 'sumalpok', 'summer.', 'summer...', 'sumosobra', 'sup', 'supercell', 'supposed', 'sure', 'sure.', 'susceptible', 'suspicious', 'susumeeee!', 'sya', 'synapse!', 'tackle', 'tadhana', 'taena', 'take', 'taken', 'takes', 'taking', 'talaga', 'talaga,', 'talaga.', 'talk', 'talking', 'tama', 'tanga', 'tanga.', 'tangent', 'tangina', 'tangina,', 'tangina.', 'tangina?', 'tanginang', 'tao', 'taon', 'tapon', 'tapos', 'tara', 'tatanga', 'tateyuu.', 'tayo', 'tayo.', 'tayo?', 'tbh.', 'technically', 'technologies', 'teens', 'teleport', 'tell', 'telling', 'tells', 'ten', 'tensor', 'tensorflow', 'testing', 'tfidf', 'than', 'thang', 'thank', 'thanks', 'thanks.', 'thanks.\"\"', 'that', \"that's\", 'that.', 'that?!', 'thay', 'the', 'the.', 'their', 'them', 'them.', 'theme.', 'then', 'then,', 'then.', 'there', \"there's\", 'there,', 'there.', 'these', 'they', 'thieves', 'thing', 'thing,', 'thing.', 'things', 'things,', 'things.', 'think', 'thinking', 'thinking.', 'this', 'this!', 'this,', 'this.', 'this?', 'tho', 'tho,', 'thonk', 'those', 'thought', 'thoughts', 'thoughts.', 'through', 'through!', 'through.', 'thrown', 'thus', 'thw', 'tied.', 'tier', 'tilt', 'time', 'time.', 'times', 'time~', 'tinimpla', 'tired', 'tired,', 'tired.', 'tired.\"', 'tl', 'to', 'today', 'today!!!!!', 'today.', 'today...', 'todo-todo', 'tomorrow...', 'tons', 'too', 'took', 'top', 'total', 'towards', 'toxic', 'toyo', 'trabaho.', 'tracker', 'trade', 'train', 'training.', 'transpired.', 'trauma', 'traumas', \"trippin'\", 'triumph', 'truck-kun.', 'truly', 'try', 'trying', 'tsaka', 'ttah', 'tulog', 'tumbling', 'tumorous.', 'tungunu', 'tungunu,', 'twee?', 'tweet', 'tweets', 'twi', 'twisted', 'twitter', 'twitter,', 'twitter.', 'twitter?', 'two', 'uhh', 'uhhh', 'uhhh.', 'ulit', 'ulo', 'ultimately', 'umay', 'umiyak', 'umulit', 'undas', 'undas.', 'under', 'underrated', 'underrated?', 'understand', 'understanding', 'understatement,', 'unfair', 'unforgettable.', 'ungo', 'ungrateful', 'uninspired.', 'uninstall', 'unironically', 'unknown', 'unorthodox', 'unpopular', 'unrequited', 'unscrewed', 'until', 'up', 'up!', 'up.', 'up?', 'upset.', 'urge', 'us.', 'use', 'used', 'useless', 'useless.', 'using', 'usual.', 'usually', 'uuvalik', 'uuwi', 'vacation', 'valentines', 'valentines.', 'valid.', 'values', 'values....', 'venting', 'very', 'via', 'vibrate', 'victim', 'viewed', 'violets', 'visualize', 'vocal', 'voice.', 'waaaaaaaaaaaaaaaruuuuuuuuuuudddddddddddooooooooooooooooo!', 'wag', 'wait', 'wait,', 'wait.', 'wait...', 'waiting', 'wake', 'waking', 'wala', 'wala.', 'walang', 'wall', 'wan', 'wanna', 'want', 'want.', 'wanted', 'wanting', 'wants', 'was', 'was.', \"wasn't\", 'watch', 'wave', 'way', 'way.', 'wcin', 'we', \"we'll\", 'web-hosting', 'weekend', 'weekend.', 'weeks', 'weird', 'weird,', 'weirded', 'well', 'well,', 'well.', 'well...', 'welp', 'welp,', 'wen', 'went', 'were', 'what', \"what's\", 'what?', 'whatever', 'wheeler,', 'when', 'where', 'which', 'whichever.', 'while', 'white.', 'who', \"who'll\", 'why', \"why'd\", 'why.', 'why?', 'wild', 'will', 'win', 'win-win', 'wind', 'wire', 'wise.', 'wish', 'witcher', 'with', 'with.', 'with...', 'with?', 'withdrawal', 'without', \"won't\", \"won't.\", 'word', 'words', 'work', 'work,', 'work.', 'work.\\xa0', 'workaholic?', 'worked!', 'worked.', 'working', 'working?', 'workmate.', 'works.', 'world', 'worlds.', 'worries', 'worry?', 'worth', 'worthwhile.', 'wotaaaaaaaaaaaan', 'would', 'wow', 'wow.', 'write', 'wrong?', 'wsssss1sd', 'wtf', 'x1', 'xd', \"xxxtentacion's\", 'y', 'y\"', \"y's\", 'ya', 'yaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaassssssssssssssssssssssssssss', 'yaaaaaaaaaaaaaaaaaaaaaaaaaaaaasssssssssssssssssssssssssssss', 'yahoo', 'yan', 'yan.', \"yanaginagi's\", 'yarn', 'yawa.', 'yay', 'yay!', 'yeah', 'yeah.', 'yeah....', 'year', 'year\"', 'year:', 'years', 'yeee', 'yeeeet', 'yeet', 'yeet.', 'yep,', 'yep.', 'yes', 'yesterday', 'yet', 'yet.', 'yoko', 'you', \"you'll\", \"you're\", 'you,', 'you.', 'you.\"', 'you?', 'young.', 'younger', 'younger.', 'your', 'yours', 'yourself\"', 'yuhs,', 'yuhssss!', 'yun', 'yun,', 'yun.', 'yun?', 'yung', 'yup.', '‚Äúhow', 'ÂçÉÊ≠≥„Åè„Çì„ÅØ„É©„É†„ÉçÁì∂„ÅÆ„Å™„Åã', 'Âñú„Å≥?', 'Â∏åÊúõ']\n"
     ]
    }
   ],
   "source": [
    "vocabfile = text\n",
    "# Create a vocabulary of characters\n",
    "vocab = sorted(set(vocabfile.split(' ')))\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to index \n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'will create llm for my tweets  salamat sa disappointment @greyunitato  salamat sa laging pag disappoint  2 am still'\n",
      "'no sign  the usual. nakalimutan na may jowa. thank you. will do the same to you.  have a social'\n",
      "'life just to spite  we angry, not sad  pretend that errthings okay  pretend like we okay  damn.'\n",
      "'i‚Äôm getting tired of solitude but whatever  let‚Äôs bottle shit up and see how long we can stay sane! :)'\n",
      "' goddamnit, i need a safe space  note to self: inarte = saloobin.  let me just mark this day'\n",
      "Input Data:  'will create llm for my tweets  salamat sa disappointment @greyunitato  salamat sa laging pag disappoint  2 am'\n",
      "Target Data:  'create llm for my tweets  salamat sa disappointment @greyunitato  salamat sa laging pag disappoint  2 am still'\n",
      "Step    0\n",
      "  input: 2425 ('will')\n",
      "  expected output: 501 ('create')\n",
      "Step    1\n",
      "  input: 501 ('create')\n",
      "  expected output: 1293 ('llm')\n",
      "Step    2\n",
      "  input: 1293 ('llm')\n",
      "  expected output: 758 ('for')\n",
      "Step    3\n",
      "  input: 758 ('for')\n",
      "  expected output: 1483 ('my')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 11:52:56.858194: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int64 and shape [7923]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-04-01 11:52:56.899700: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int64 and shape [7923]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "len(vocab)\n",
    "\n",
    "text_to_int = np.array([char2idx[x] for x in text.split(' ')])\n",
    "\n",
    "# Parse to train per sequence\n",
    "sequence_length = 20\n",
    "examples_per_epoch = len(text.split(' ')) //sequence_length\n",
    "\n",
    "# Slice dataset as a tensor\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_to_int)\n",
    "\n",
    "# Cut for desirable length \n",
    "sequences = char_dataset.batch(sequence_length+1, drop_remainder=True)\n",
    "for item in sequences.take(5):\n",
    "    print(repr(' '.join(idx2char[item.numpy()])))\n",
    "\n",
    "# Function to split dataset {FOR TRAINING}: input and next target character\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    \n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "# Example Training\n",
    "for iExample, tExample in dataset.take(1):\n",
    "    print('Input Data: ', repr(' '.join(idx2char[iExample.numpy()])))\n",
    "    print('Target Data: ', repr(' '.join(idx2char[tExample.numpy()])))\n",
    "    for i, (input_idx, target_idx) in enumerate(zip(iExample[:4], tExample[:4])):\n",
    "        print(\"Step {:4d}\".format(i))\n",
    "        print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "        print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_9922/1695046745.py:16: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 11:52:57.284231: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-04-01 11:52:57.286116: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-04-01 11:52:57.287356: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize size of batch for training: The input node layer\n",
    "batch_size = 8\n",
    "steps_per_epoch = examples_per_epoch//batch_size\n",
    "\n",
    "# Size of buffer\n",
    "buffer_size = 1000\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "# 1st Layer of hidden Node\n",
    "embedding_dim = 256 \n",
    "# Recursive Layer \n",
    "rnn_units = 1024\n",
    "\n",
    "# Initialize RNN Layer if for GPU or CPU\n",
    "if tf.test.is_gpu_available():\n",
    "    rnn = tf.keras.layers.CuDNNGRU\n",
    "else:\n",
    "    import functools\n",
    "    rnn = functools.partial(\n",
    "    tf.keras.layers.GRU, recurrent_activation='sigmoid')\n",
    "\n",
    "# Build model for initializing\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        rnn(rnn_units,\n",
    "           return_sequences=True,\n",
    "           recurrent_initializer='glorot_uniform',\n",
    "           stateful=True),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Building Model\n",
    "model = build_model(vocab_size=len(vocab),\n",
    "                   embedding_dim=embedding_dim,\n",
    "                   rnn_units=rnn_units,\n",
    "                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 20, 2528) # (batch_size, sequence_length, vocab_size)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (8, None, 256)            647168    \n",
      "                                                                 \n",
      " gru (GRU)                   (8, None, 1024)           3938304   \n",
      "                                                                 \n",
      " dense (Dense)               (8, None, 2528)           2591200   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,176,672\n",
      "Trainable params: 7,176,672\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 11:52:57.526226: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int64 and shape [7923]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-04-01 11:52:57.526610: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int64 and shape [7923]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "# Model summary\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "model.summary()\n",
    "\n",
    "# Samples \n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Shape:  (8, 20, 2528) # (batch_size, sequence_length, vocab_size)\n",
      "Scalar Loss:  7.835019\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 11:52:58.154678: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int64 and shape [7923]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-04-01 11:52:58.155075: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int64 and shape [7923]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-04-01 11:52:58.334443: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-04-01 11:52:58.335902: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-04-01 11:52:58.337233: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-04-01 11:52:58.873830: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-04-01 11:52:58.875549: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-04-01 11:52:58.876678: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 17s 327ms/step - loss: 7.4674\n",
      "Epoch 2/50\n",
      "49/49 [==============================] - 16s 330ms/step - loss: 6.4544\n",
      "Epoch 3/50\n",
      "49/49 [==============================] - 16s 325ms/step - loss: 5.9704\n",
      "Epoch 4/50\n",
      "49/49 [==============================] - 16s 327ms/step - loss: 5.4784\n",
      "Epoch 5/50\n",
      "49/49 [==============================] - 16s 332ms/step - loss: 4.9378\n",
      "Epoch 6/50\n",
      "49/49 [==============================] - 17s 340ms/step - loss: 4.4296\n",
      "Epoch 7/50\n",
      "49/49 [==============================] - 16s 331ms/step - loss: 3.7844\n",
      "Epoch 8/50\n",
      "49/49 [==============================] - 16s 324ms/step - loss: 3.0680\n",
      "Epoch 9/50\n",
      "49/49 [==============================] - 16s 325ms/step - loss: 2.3626\n",
      "Epoch 10/50\n",
      "49/49 [==============================] - 16s 327ms/step - loss: 1.7096\n",
      "Epoch 11/50\n",
      "49/49 [==============================] - 16s 324ms/step - loss: 1.2090\n",
      "Epoch 12/50\n",
      "49/49 [==============================] - 16s 332ms/step - loss: 0.8674\n",
      "Epoch 13/50\n",
      "49/49 [==============================] - 16s 332ms/step - loss: 0.6543\n",
      "Epoch 14/50\n",
      "49/49 [==============================] - 16s 321ms/step - loss: 0.4963\n",
      "Epoch 15/50\n",
      "49/49 [==============================] - 16s 325ms/step - loss: 0.4014\n",
      "Epoch 16/50\n",
      "49/49 [==============================] - 16s 330ms/step - loss: 0.3496\n",
      "Epoch 17/50\n",
      "49/49 [==============================] - 16s 326ms/step - loss: 0.2987\n",
      "Epoch 18/50\n",
      "49/49 [==============================] - 16s 323ms/step - loss: 0.2732\n",
      "Epoch 19/50\n",
      "49/49 [==============================] - 16s 334ms/step - loss: 0.2360\n",
      "Epoch 20/50\n",
      "49/49 [==============================] - 16s 325ms/step - loss: 0.2212\n",
      "Epoch 21/50\n",
      "49/49 [==============================] - 16s 324ms/step - loss: 0.2102\n",
      "Epoch 22/50\n",
      "49/49 [==============================] - 16s 319ms/step - loss: 0.1914\n",
      "Epoch 23/50\n",
      "49/49 [==============================] - 16s 317ms/step - loss: 0.1738\n",
      "Epoch 24/50\n",
      "49/49 [==============================] - 16s 318ms/step - loss: 0.1680\n",
      "Epoch 25/50\n",
      "49/49 [==============================] - 16s 319ms/step - loss: 0.1636\n",
      "Epoch 26/50\n",
      "49/49 [==============================] - 16s 325ms/step - loss: 0.1518\n",
      "Epoch 27/50\n",
      "49/49 [==============================] - 16s 319ms/step - loss: 0.1512\n",
      "Epoch 28/50\n",
      "49/49 [==============================] - 16s 325ms/step - loss: 0.1444\n",
      "Epoch 29/50\n",
      "49/49 [==============================] - 15s 313ms/step - loss: 0.1409\n",
      "Epoch 30/50\n",
      "49/49 [==============================] - 16s 322ms/step - loss: 0.1369\n",
      "Epoch 31/50\n",
      "49/49 [==============================] - 15s 316ms/step - loss: 0.1257\n",
      "Epoch 32/50\n",
      "49/49 [==============================] - 15s 310ms/step - loss: 0.1291\n",
      "Epoch 33/50\n",
      "49/49 [==============================] - 16s 323ms/step - loss: 0.1201\n",
      "Epoch 34/50\n",
      "49/49 [==============================] - 16s 319ms/step - loss: 0.1313\n",
      "Epoch 35/50\n",
      "49/49 [==============================] - 16s 320ms/step - loss: 0.1192\n",
      "Epoch 36/50\n",
      "49/49 [==============================] - 15s 310ms/step - loss: 0.1213\n",
      "Epoch 37/50\n",
      "49/49 [==============================] - 16s 324ms/step - loss: 0.1124\n",
      "Epoch 38/50\n",
      "49/49 [==============================] - 16s 331ms/step - loss: 0.1196\n",
      "Epoch 39/50\n",
      "49/49 [==============================] - 16s 326ms/step - loss: 0.1242\n",
      "Epoch 40/50\n",
      "49/49 [==============================] - 16s 324ms/step - loss: 0.1188\n",
      "Epoch 41/50\n",
      "49/49 [==============================] - 15s 315ms/step - loss: 0.1266\n",
      "Epoch 42/50\n",
      "49/49 [==============================] - 15s 314ms/step - loss: 0.1179\n",
      "Epoch 43/50\n",
      "49/49 [==============================] - 16s 321ms/step - loss: 0.1209\n",
      "Epoch 44/50\n",
      "49/49 [==============================] - 16s 321ms/step - loss: 0.1318\n",
      "Epoch 45/50\n",
      "49/49 [==============================] - 16s 323ms/step - loss: 0.1312\n",
      "Epoch 46/50\n",
      "49/49 [==============================] - 15s 315ms/step - loss: 0.1169\n",
      "Epoch 47/50\n",
      "49/49 [==============================] - 16s 322ms/step - loss: 0.1165\n",
      "Epoch 48/50\n",
      "49/49 [==============================] - 16s 316ms/step - loss: 0.1169\n",
      "Epoch 49/50\n",
      "49/49 [==============================] - 15s 316ms/step - loss: 0.1225\n",
      "Epoch 50/50\n",
      "49/49 [==============================] - 16s 317ms/step - loss: 0.1145\n"
     ]
    }
   ],
   "source": [
    "# Calculates Loss\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction Shape: \", example_batch_predictions.shape,\"# (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Scalar Loss: \", example_batch_loss.numpy().mean())\n",
    "\n",
    "# Compile model \n",
    "model.compile(optimizer=tf.optimizers.Adam(), loss=loss)\n",
    "\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "# Model Training \n",
    "# Callbacks are used to refit shape of model\n",
    "history = model.fit(dataset.repeat(), epochs=50, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._iterations\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 12:13:26.625254: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-04-01 12:13:26.627072: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-04-01 12:13:26.628338: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model.save('../Model/twitter.word.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  \n",
    "  num_generate = 50\n",
    "  \n",
    "  input_eval = [char2idx[s] for s in start_string.split(' ')]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "  \n",
    "  print(input_eval)\n",
    "  \n",
    "  text_generated = []\n",
    "  \n",
    "  temperature = 1.0\n",
    "  \n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "    predictions = model(input_eval)\n",
    "    \n",
    "    predictions = tf.squeeze(predictions, 0)\n",
    "    \n",
    "    predictions = predictions/ temperature\n",
    "    \n",
    "    predicted_id = tf.compat.v1.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "    \n",
    "    input_eval = tf.expand_dims([predicted_id],0)\n",
    "    \n",
    "    text_generated.append(idx2char[predicted_id])\n",
    "    \n",
    "  return (start_string + ' '.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
      "roses are red; violets are blue;  on september 27;  i'm going to you.  \"ang childish mo\" oh my fuckin god  i knew getting tired of solitude but whatever  let‚Äôs bottle shit up and see how long we can stay sane! :)  tangina ang sakit\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 12:14:17.997959: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-04-01 12:14:17.999733: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-04-01 12:14:18.000739: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "loaded_model = tf.keras.saving.load_model(\"../Model/twitter.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'re'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(generate_text(loaded_model, start_string\u001b[39m=\u001b[39;49m\u001b[39mu\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mre\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(model, start_string)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_text\u001b[39m(model, start_string):\n\u001b[1;32m      3\u001b[0m   num_generate \u001b[39m=\u001b[39m \u001b[39m50\u001b[39m\n\u001b[0;32m----> 5\u001b[0m   input_eval \u001b[39m=\u001b[39m [char2idx[s] \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m start_string\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[1;32m      6\u001b[0m   input_eval \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mexpand_dims(input_eval, \u001b[39m0\u001b[39m)\n\u001b[1;32m      8\u001b[0m   \u001b[39mprint\u001b[39m(input_eval)\n",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_text\u001b[39m(model, start_string):\n\u001b[1;32m      3\u001b[0m   num_generate \u001b[39m=\u001b[39m \u001b[39m50\u001b[39m\n\u001b[0;32m----> 5\u001b[0m   input_eval \u001b[39m=\u001b[39m [char2idx[s] \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m start_string\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[1;32m      6\u001b[0m   input_eval \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mexpand_dims(input_eval, \u001b[39m0\u001b[39m)\n\u001b[1;32m      8\u001b[0m   \u001b[39mprint\u001b[39m(input_eval)\n",
      "\u001b[0;31mKeyError\u001b[0m: 're'"
     ]
    }
   ],
   "source": [
    "print(generate_text(loaded_model, start_string=u\"re\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
